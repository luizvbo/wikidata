{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Loading Wikipedia articles (EN) with Python\n",
    "date: 2020-02-17\n",
    "\n",
    "# Put any other Academic metadata here...\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia is the largest and most popular general reference work on the World Wide Web, and is one of the most popular websites ranked by Alexa as of January 2020. \n",
    "\n",
    "As of February 2020, there are 6,016,720 articles in the English Wikipedia containing over 3.5 billion words. There are a lot of information about the amount of data in Wikipedia that can be found in [this article](https://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia).\n",
    "\n",
    "![Content page count of the English-language Wikipedia from the beginning to 2019-03-21](https://upload.wikimedia.org/wikipedia/commons/1/19/Size_of_English_Wikipedia_graph_2019-03-01.png)\n",
    ">Content page count of the English-language Wikipedia from the beginning to 2019-03-21\n",
    "\n",
    "**This article is going to show you how to download the whole Wikipedia (in English) and load the data in Python.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Data\n",
    "\n",
    "The fastest way of getting Wikipedia is though torrent. Download the last version of the data dump from this [page](https://meta.wikimedia.org/wiki/Data_dump_torrents#English_Wikipedia). You should download the newest version of the file `enwiki-YYYYMMDD-pages-articles-multistream.xml.bz2`, where `YYYYMMDD` is the date of the dump.\n",
    "\n",
    "The MD5 for the file can be found in the link http://ftp.acc.umu.se/mirror/wikimedia.org/dumps/enwiki/YYYYMMDD/md5sums-enwiki-YYYYMMDD-pages-articles-multistream.xml.bz2.txt where `YYYYMMDD` should be replace by the date of the dump.\n",
    "\n",
    "After downloading the articles **bz2** file, we need to download the list on indices for the articles from http://ftp.acc.umu.se/mirror/wikimedia.org/dumps/enwiki/YYYYMMDD/enwiki-YYYYMMDD-pages-articles-multistream-index.txt.bz2. Again, replace `YYYYMMDD` by the date of the dump.\n",
    "\n",
    "\n",
    "After downloading the files you should extract **only the index** file. On Linux, we can use `lbzip2` to uncompress the file using multiple CPUs, speeding up the process. In the terminal, in the file folder type:\n",
    "\n",
    "```bash\n",
    "$ lbzip2 -d enwiki-YYYYMMDD-pages-articles-multistream-index.txt.bz2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "Now is where the things start to become interesting. Since the file is too large to fit in memory, we are going to load it iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "import io\n",
    "\n",
    "from tqdm import tqdm\n",
    "from lxml import etree\n",
    "from bz2 import BZ2Decompressor\n",
    "from typing import (\n",
    "    List, Generator\n",
    ")\n",
    "\n",
    "path_articles = 'enwiki-20200201-pages-articles-multistream.xml.bz2'\n",
    "path_index = 'enwiki-20200201-pages-articles-multistream-index.txt'\n",
    "path_index_clean = 'enwiki-20200201-pages-articles-multistream-index_clean.txt'\n",
    "path_wiki_parquet = 'wikipedia.parquet'\n",
    "n_processes = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multistream dump file contains multiple bz2 'streams' (bz2 header, body, footer) concatenated together into one file, in contrast to the vanilla file which contains one stream. Each separate 'stream' (or really, file) in the multistream dump contains 100 pages, except possibly the last one. The multistream file allows you to get an article from the archive without unpacking the whole thing. \n",
    "\n",
    "The index file contains the full list of articles. The first field of this index is the number of bytes to seek into the compressed archive pages-articles-multistream.xml.bz2, the second is the article ID, the third the article title. A colon (`:`) is used to separate fields.\n",
    "\n",
    "Since we would like to extract all the articles from wikipedia, we don't have to keep track of titles and IDs, only the offsets. Thus, we read the offsets and store them into a new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_offsets(path_index: str, path_index_clean: str) -> List[int]:\n",
    "    \"\"\"Get page offsets from wikipedia file or cached version\n",
    "\n",
    "    Wikipedia provide an index file containing the list of articles with their\n",
    "    respective id and offset from the start of the file. Since we are\n",
    "    interested only on the offsets, we read the original file, provided by\n",
    "    `path_index`, extract the offsets and store in another file (defined by\n",
    "    `path_index_clean`) to speed up the process\n",
    "\n",
    "    Args:\n",
    "        path_index (str): Path to the original index file provided by Wikipedia\n",
    "        path_index_clean (str): Path to our version, containing only offsets\n",
    "\n",
    "    Returns:\n",
    "        List[int]: List of offsets\n",
    "    \"\"\"\n",
    "    # Get the list of offsets\n",
    "    # If our new offset file was not created, it gets the information\n",
    "    # from the index file\n",
    "    if not os.path.isfile(path_index_clean):\n",
    "        # Read the byte offsets from the index file\n",
    "        page_offset = []\n",
    "        last_offset = None\n",
    "        with open(path_index, 'r') as f:\n",
    "            for line in tqdm(f):\n",
    "                offset = line.split(':', 1)[0]\n",
    "                if last_offset != offset:\n",
    "                    last_offset = offset\n",
    "                    page_offset.append(int(offset))\n",
    "\n",
    "        with open(path_index_clean, 'w') as f:\n",
    "            f.write(','.join([str(i) for i in page_offset]))\n",
    "    else:\n",
    "        with open(path_index_clean, 'r') as f:\n",
    "            page_offset = [int(idx) for idx in f.read().split(',')]\n",
    "\n",
    "    return page_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bz2_byte_str(path_articles: str,\n",
    "                     offset_list: List[int]) -> Generator[bytes, None, None]:\n",
    "    \"\"\"Read the multistream bz2 file using the offset list\n",
    "\n",
    "    The offset list defines where the bz2 (sub)file starts and ends\n",
    "\n",
    "    Args:\n",
    "        path_articles (str): Path to the bz2 file containing the Wikipedia\n",
    "            articles.\n",
    "        offset_list (List[int]): List of byte offsets\n",
    "\n",
    "    Yields:\n",
    "        bytes: String of bytes corresponding to a set of articles compressed\n",
    "    \"\"\"\n",
    "    with open(path_articles, \"rb\") as f:\n",
    "        last_offset = offset_list[0]\n",
    "        # Drop the data before the offset\n",
    "        f.read(last_offset)\n",
    "        for next_offset in offset_list[1:]:\n",
    "            offset = next_offset - last_offset\n",
    "            last_offset = next_offset\n",
    "            yield f.read(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles(byte_string_compressed: bytes) -> pd.DataFrame:\n",
    "    \"\"\"Get a dataframe containing the set of articles from a bz2\n",
    "\n",
    "    Args:\n",
    "        byte_string_compressed (bytes): Byte string corresponding to the bz2\n",
    "            stream\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with columns title and article\n",
    "    \"\"\"\n",
    "    def _get_text(list_xml_el):\n",
    "        \"\"\"Return the list of content for a list of xml_elements\"\"\"\n",
    "        return [el.text for el in list_xml_el]\n",
    "\n",
    "    def _get_id(list_xml_el):\n",
    "        \"\"\"Return the list of id's for a list of xml_elements\"\"\"\n",
    "        return [int(el.text) for el in list_xml_el]\n",
    "\n",
    "    bz2d = BZ2Decompressor()\n",
    "    byte_string = bz2d.decompress(byte_string_compressed)\n",
    "    doc = etree.parse(io.BytesIO(b'<root> ' + byte_string + b' </root>'))\n",
    "\n",
    "    col_id = _get_text(doc.xpath('*/id'))\n",
    "    col_title = _get_text(doc.xpath('*/title'))\n",
    "    col_article = _get_text(doc.xpath('*/revision/text'))\n",
    "\n",
    "    return pd.DataFrame([col_title, col_article], columns=col_id, index=['title', 'article']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 5270/199685 [02:19<52:21, 61.89it/s]"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "queue = []\n",
    "page_offset = get_page_offsets(path_index, path_index_clean)\n",
    "# Read the file sequentially\n",
    "for bit_str in tqdm(get_bz2_byte_str(path_articles, page_offset), total=len(page_offset)):\n",
    "    # Feed the queue\n",
    "    if len(queue) < n_processes * 6:\n",
    "        queue.append(bit_str)\n",
    "    \n",
    "    # Decompress and extract the infomation in parallel\n",
    "    else:\n",
    "        with Pool(processes=n_processes) as pool:\n",
    "            df = pd.concat(pool.imap_unordered(get_articles, queue))\n",
    "        # Clear the queue\n",
    "        queue.clear()\n",
    "\n",
    "        # Create a parquet table from your dataframe\n",
    "        table = pa.Table.from_pandas(df)\n",
    "\n",
    "        # Write direct to your parquet file\n",
    "        pq.write_to_dataset(table , root_path=path_wiki_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the XML string\n",
    "\n",
    "The XML structure can be fond on this [page](https://meta.wikimedia.org/wiki/Data_dumps/Dump_format)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
